{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftomovski/hw2.github.io/blob/main/classdemo/TrainingDynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Training Dynamics\n",
        "\n",
        "This notebook sets up a tiny training problem to demonstrate many of the techniques that are important in getting neural network training to work well in practical situations.\n",
        "\n",
        "It shows you how to\n",
        "* Run an ordinary neural network training loop.\n",
        "* Monitor and plot performance on held-out data, to understand and fix lack of convergence or overfitting.\n",
        "* Examine and plot gradient distributions, to understand and fix vanishing or exploding gradients.\n",
        "* Experiment with different nonlinearities, optimizers, losses, regularizers, and neural network architectures."
      ],
      "metadata": {
        "id": "k8_g86w_2emG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLMWqjkyNj5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, torch\n",
        "train_data, train_labels, test_data, test_labels = [\n",
        "    torch.tensor(m[k]).float()\n",
        "    for m in [numpy.load('tiny-classification.npz')]\n",
        "    for k in 'train_data train_labels val_data val_labels'.split()]\n",
        "\n",
        "print(f'The training data has {train_data.size(0)} samples, each a vector of {train_data.size(1)} numbers along with')\n",
        "print(f'a corresponding set of {train_labels.size(0)} labels, assigning {train_labels.min()} or {train_labels.max()} to each sample.')\n",
        "\n",
        "print(f'The test data has {test_data.size(0)} samples and labels that are disjoint from the training data.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS3QaLy_PQzr",
        "outputId": "0b654828-19ff-4be0-e6de-77c1547e8748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training data has 8000 samples, each a vector of 36 numbers along with\n",
            "a corresponding set of 8000 labels, assigning 0.0 or 1.0 to each sample.\n",
            "The test data has 1000 samples and labels that are disjoint from the training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download training data."
      ],
      "metadata": {
        "id": "syRQ1ldqQKlY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROKVnRPDIr8A"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget -r https://ds4440.baulab.info/data/tiny-classification.npz\n",
        "!wget -r https://ds4440.baulab.info/data/hard-classification.npz\n",
        "!pip install git+https://github.com/davidbau/baukit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a \"Supervise\" module\n",
        "\n",
        "It evaluates a loss over a network given pairs of input and target output data.\n",
        "\n",
        "The code should look like this.  It just hold on to a network and a loss function that it calls the \"criterion\".\n",
        "\n",
        "Then when it is given both input and output, it runs the network on the input; then it checks the \"criterion\" to compare the output to the target output data."
      ],
      "metadata": {
        "id": "Q3E_ZbRBQZgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class Supervise(nn.Module):\n",
        "    def __init__(self, criterion, net):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.criterion = criterion\n",
        "    def forward(self, x, y):\n",
        "        out = self.net(x).squeeze()\n",
        "        return self.criterion(out, y)"
      ],
      "metadata": {
        "id": "WiBWIewdQZ8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a supervised neural network\n",
        "\n",
        "This one has a few linear layers, each followed by a Sigmoid."
      ],
      "metadata": {
        "id": "yTRWQsQXSxQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "input_size = 36\n",
        "hidden_dims = 64\n",
        "output_dims = 1\n",
        "\n",
        "net = Supervise(\n",
        "        nn.MSELoss(),\n",
        "        nn.Sequential(OrderedDict([\n",
        "            ('layer1', nn.Linear(input_size, hidden_dims)),\n",
        "            ('sigma1', nn.Sigmoid()),\n",
        "            ('layer2', nn.Linear(hidden_dims, hidden_dims)),\n",
        "            ('sigma2', nn.Sigmoid()),\n",
        "            ('layer3', nn.Linear(hidden_dims, hidden_dims)),\n",
        "            ('sigma3', nn.Sigmoid()),\n",
        "            ('layer4', nn.Linear(hidden_dims, hidden_dims)),\n",
        "            ('sigma4', nn.Sigmoid()),\n",
        "            ('layer5', nn.Linear(hidden_dims, output_dims)),\n",
        "            ('sigma5', nn.Sigmoid())\n",
        "        ]))\n",
        "    )"
      ],
      "metadata": {
        "id": "OYy4zPdwP6Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the network\n",
        "\n",
        "Here is a typical training loop.\n",
        "\n",
        "It is very minimal right now.  Let's add some visualization code to it."
      ],
      "metadata": {
        "id": "pH63w1Ryw0oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "num_iterations = 200\n",
        "test_every = 10\n",
        "\n",
        "for epoch in range(200):\n",
        "    loss = net(train_data.float(), train_labels.float())\n",
        "    net.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()  # Update model parameters using the optimizer's update rule\n"
      ],
      "metadata": {
        "id": "9Ki7mNeew4OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding visualization code\n",
        "\n",
        "We will examine the basic training loop above together.\n",
        "\n",
        "What it's missing is any code that might allow us to debug how well (or badly) our training is going\n",
        "\n",
        "Add the following visualization code:\n",
        "\n",
        "### Tracking Peformance in Arrays\n",
        "\n",
        "Before the training loop:\n",
        "```\n",
        "train_losses, train_accs, test_accs = [], [], []\n",
        "```\n",
        "\n",
        "\n",
        "Inside the training loop, add a track of all the training losses.\n",
        "```\n",
        "    train_losses.append([epoch, loss.item()])\n",
        "```\n",
        "\n",
        "\n",
        "### Testing performance on both training and held-out test data\n",
        "\n",
        "And then also some testing code.\n",
        "\n",
        "Why is it important to test on different data?\n",
        "\n",
        "```\n",
        "    if epoch % test_every == test_every - 1:\n",
        "        grads = torch.stack([p.grad.abs().max() for p in net.parameters()])\n",
        "        maxg, ming = grads.abs().max(), grads.abs().min()\n",
        "        net.eval()\n",
        "        train_outputs = net.net(train_data.float())\n",
        "        net.train()\n",
        "        train_preds = (train_outputs.squeeze() > 0.5).float()\n",
        "        train_accuracy = (train_preds == train_labels).float().mean()\n",
        "        train_accs.append([epoch + 1, train_accuracy])\n",
        "        test_outputs = net.net(test_data.float())\n",
        "        test_preds = (test_outputs.squeeze() > 0.5).float()\n",
        "        test_accuracy = (test_preds == test_labels).float().mean()\n",
        "        test_accs.append([epoch + 1, test_accuracy])\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}, Loss: {loss.item():.5f}, Grad range {maxg:.1e} to {ming:.1e}, \"\n",
        "            f\"Train Accuracy: {train_accuracy.item()}, Test Accuracy: {test_accuracy.item()}\")\n",
        "        if test_accuracy.item() == 1.0:\n",
        "            break\n",
        "```\n",
        "\n",
        "### Adding some graphing code to visualize\n",
        "\n",
        "After the training loop, add this.\n",
        "\n",
        "```\n",
        "# Test the Model\n",
        "with torch.no_grad():\n",
        "    train_outputs = net.net(train_data.float())\n",
        "    train_preds = (train_outputs.squeeze() > 0.5).float()\n",
        "    train_accuracy = (train_preds == train_labels).float().mean()\n",
        "    test_outputs = net.net(test_data.float())\n",
        "    test_preds = (test_outputs.squeeze() > 0.5).float()\n",
        "    test_accuracy = (test_preds == test_labels).float().mean()\n",
        "    print(\n",
        "        f\"\\nTrain Accuracy: {train_accuracy.item():.5f}, Test Accuracy: {test_accuracy.item():.5f}\"\n",
        "    )\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots()\n",
        "ax2 = ax.twinx()\n",
        "ax.plot(*zip(*train_losses), label=\"Training loss\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax2.plot(*zip(*train_accs), color=\"orange\", label=\"Training accuracy\")\n",
        "ax2.plot(*zip(*test_accs), color=\"red\", label=\"Test accuracy\")\n",
        "ax2.set_ylim(0.0, 1.0)\n",
        "for a in [ax, ax2]:\n",
        "    for pos in \"top right bottom left\".split():\n",
        "        a.spines[pos].set_visible(False)\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Loss\")\n",
        "ax2.set_ylabel(\"Accuracy\")\n",
        "fig.legend(loc=\"lower left\", bbox_to_anchor=(0, 0), bbox_transform=ax.transAxes)\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "The code above is similar to the homework setup."
      ],
      "metadata": {
        "id": "mKFjYieizYJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting gradients\n",
        "\n",
        "A common cause of \"frozen training\" is that derivatives go to near zero.\n",
        "\n",
        "Let's plot the derivatives to see what they look like for our network.\n",
        "\n",
        "Here I am using a package called `baukit` has a Trace utility that keeps copies of all the activations while we run the network."
      ],
      "metadata": {
        "id": "U_ZJ57-zzmiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from baukit import TraceDict\n",
        "\n",
        "with TraceDict(net, [n for n, _ in net.named_modules()], retain_grad=True) as trace:\n",
        "    loss = net(train_data[0], train_labels[0])\n",
        "net.zero_grad()\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "1ESz285ESZEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is a nice histogram plotting utility."
      ],
      "metadata": {
        "id": "MyaJ3j6j4jrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def plot_histograms(title, datalist):\n",
        "    fig, axes = plt.subplots(len(datalist), 1, figsize=(10, 1.5 * len(datalist)), sharex=True)\n",
        "    fig.suptitle(title)\n",
        "    for i, (name, data) in enumerate(datalist):\n",
        "        axes[i].hist(data.flatten().detach().numpy(), bins=100)\n",
        "        axes[i].set_title(name)\n",
        "    fig.tight_layout()\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "m3-MybVhreZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting parameter gradients\n",
        "\n"
      ],
      "metadata": {
        "id": "NbOv2khB4n6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist = [(n, p.grad)\n",
        "    for n, p in net.named_parameters() if 'weight' in n]\n",
        "plot_histograms('Parameter gradients', datalist)"
      ],
      "metadata": {
        "id": "n7D8V4cZSqTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting neuron activations and their gradients\n",
        "\n",
        "Let's discuss exactly where these derivatives are in the backpropagation."
      ],
      "metadata": {
        "id": "CL29tvPb4t2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist = [(n, trace[n].output)\n",
        "    for n, p in net.named_modules() if 'layer' in n]\n",
        "plot_histograms('Activations', datalist[:-1])\n",
        "\n",
        "\n",
        "datalist = [(n, trace[n].output.grad)\n",
        "    for n, p in net.named_modules() if 'layer' in n]\n",
        "plot_histograms('Activation gradients', datalist[:-1])"
      ],
      "metadata": {
        "id": "xNyhCALDLjBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make an experimental harness\n",
        "\n",
        "Now we will experiment with all these things by making a little test harness that does all the above on a network that we choose."
      ],
      "metadata": {
        "id": "hrzTc_9C0Dy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test(net, opt=lambda x: torch.optim.SGD(x, lr=0.01), num_iterations=200, test_every=10):\n",
        "    # Set up the Loss Function and Optimizer\n",
        "    optimizer = opt(\n",
        "        net.parameters()\n",
        "    )  # Initialize the optimizer with model parameters\n",
        "    print(f\"{sum([p.numel() for p in net.parameters()])} parameters\")\n",
        "    train_losses, train_accs, test_accs = [], [], []\n",
        "\n",
        "    for epoch in range(num_iterations):\n",
        "        loss = net(train_data.float(), train_labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append([epoch, loss.item()])\n",
        "        optimizer.step()  # Update model parameters using the optimizer's update rule\n",
        "        if epoch % test_every == test_every - 1:\n",
        "            grads = torch.stack([p.grad.abs().max() for p in net.parameters()])\n",
        "            maxg, ming = grads.abs().max(), grads.abs().min()\n",
        "            net.eval()\n",
        "            train_outputs = net.net(train_data.float())\n",
        "            net.train()\n",
        "            train_preds = (train_outputs.squeeze() > 0.5).float()\n",
        "            train_accuracy = (train_preds == train_labels).float().mean()\n",
        "            train_accs.append([epoch + 1, train_accuracy])\n",
        "            test_outputs = net.net(test_data.float())\n",
        "            test_preds = (test_outputs.squeeze() > 0.5).float()\n",
        "            test_accuracy = (test_preds == test_labels).float().mean()\n",
        "            test_accs.append([epoch + 1, test_accuracy])\n",
        "            print(\n",
        "                f\"Epoch {epoch+1}, Loss: {loss.item():.5f}, Grad range {maxg:.1e} to {ming:.1e}, \"\n",
        "                f\"Train Accuracy: {train_accuracy.item()}, Test Accuracy: {test_accuracy.item()}\",\n",
        "                end=\"   \\r\",\n",
        "            )\n",
        "            if test_accuracy.item() == 1.0:\n",
        "                break\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Test the Model\n",
        "    with torch.no_grad():\n",
        "        train_outputs = net.net(train_data.float())\n",
        "        train_preds = (train_outputs.squeeze() > 0.5).float()\n",
        "        train_accuracy = (train_preds == train_labels).float().mean()\n",
        "        test_outputs = net.net(test_data.float())\n",
        "        test_preds = (test_outputs.squeeze() > 0.5).float()\n",
        "        test_accuracy = (test_preds == test_labels).float().mean()\n",
        "        print(\n",
        "            f\"\\nTrain Accuracy: {train_accuracy.item():.5f}, Test Accuracy: {test_accuracy.item():.5f}\"\n",
        "        )\n",
        "\n",
        "    # Visualization\n",
        "    if len(train_losses) > 0:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax2 = ax.twinx()\n",
        "        ax.plot(*zip(*train_losses), label=\"Training loss\")\n",
        "        ax.set_yscale(\"log\")\n",
        "        ax2.plot(*zip(*train_accs), color=\"orange\", label=\"Training accuracy\")\n",
        "        ax2.plot(*zip(*test_accs), color=\"red\", label=\"Test accuracy\")\n",
        "        ax2.set_ylim(0.0, 1.0)\n",
        "        for a in [ax, ax2]:\n",
        "            for pos in \"top right bottom left\".split():\n",
        "                a.spines[pos].set_visible(False)\n",
        "        ax.set_xlabel(\"Epochs\")\n",
        "        ax.set_ylabel(\"Loss\")\n",
        "        ax2.set_ylabel(\"Accuracy\")\n",
        "        fig.legend(loc=\"lower left\", bbox_to_anchor=(0, 0), bbox_transform=ax.transAxes)\n",
        "        fig.show()\n",
        "\n",
        "    # One last pass, for plotting gradients\n",
        "    with TraceDict(net, [n for n, _ in net.named_modules() if 'layer' in n],\n",
        "                   retain_grad=True) as trace:\n",
        "        loss = net(train_data[0], train_labels[0])\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "    plot_histograms('Parameter gradients', [(n, p.grad)\n",
        "        for n, p in net.named_parameters() if 'weight' in n])\n",
        "\n",
        "    plot_histograms('Activations', [(n, trace[n].output)\n",
        "        for n, p in net.named_modules() if 'layer' in n])\n",
        "\n",
        "    plot_histograms('Activation gradients', [(n, trace[n].output.grad)\n",
        "        for n, p in net.named_modules() if 'layer' in n])"
      ],
      "metadata": {
        "id": "90T1_FeB0DIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_test(Supervise(\n",
        "        nn.MSELoss(),\n",
        "        nn.Sequential(OrderedDict([\n",
        "            ('layer1', nn.Linear(input_size, hidden_dims)),\n",
        "            ('sigma1', nn.Sigmoid()),\n",
        "            ('layer2', nn.Linear(hidden_dims, hidden_dims)),\n",
        "            ('sigma2', nn.Sigmoid()),\n",
        "            ('layer3', nn.Linear(hidden_dims, hidden_dims)),\n",
        "            ('sigma3', nn.Sigmoid()),\n",
        "            ('layer4', nn.Linear(hidden_dims, hidden_dims)),\n",
        "            ('sigma4', nn.Sigmoid()),\n",
        "            ('layer5', nn.Linear(hidden_dims, output_dims)),\n",
        "            ('sigma5', nn.Sigmoid())\n",
        "        ]))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "gTF7BzqS1Q1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now: experiment.\n",
        "\n",
        "Depending on time:\n",
        "\n",
        "* Try Tanh vs Sigmoid vs ReLU nonlinearities\n",
        "* Try Cross-Entropy instead of MSE loss\n",
        "* Try different initizations\n",
        "* Try different learning rates\n",
        "* Try ADAM instead of SGD\n",
        "* Try adding Weight Decay to reduce overfitting\n",
        "* Try Residuals and Batchnorms"
      ],
      "metadata": {
        "id": "pjj2gdgE2Dr5"
      }
    }
  ]
}